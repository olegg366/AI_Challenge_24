{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подключение библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Soft\\anaconda3\\envs\\aijc\\lib\\site-packages\\theano\\scalar\\basic.py:2323: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  self.ctor = getattr(np, o_type.dtype)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Activation, Embedding, Input\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtheano\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mT\u001b[39;00m   \n",
      "File \u001b[1;32md:\\Soft\\anaconda3\\envs\\aijc\\lib\\site-packages\\theano\\__init__.py:124\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtheano\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msafe_asarray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _asarray\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtheano\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprinting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pprint, pp\n\u001b[1;32m--> 124\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtheano\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscan_module\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (scan, \u001b[38;5;28mmap\u001b[39m, reduce, foldl, foldr, clone,\n\u001b[0;32m    125\u001b[0m                                 scan_checkpoints)\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtheano\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mupdates\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrderedUpdates\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# scan_module import above initializes tensor and scalar making these imports\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# redundant\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m \n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m# import sparse\u001b[39;00m\n",
      "File \u001b[1;32md:\\Soft\\anaconda3\\envs\\aijc\\lib\\site-packages\\theano\\scan_module\\__init__.py:41\u001b[0m\n\u001b[0;32m     38\u001b[0m __copyright__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(c) 2010, Universite de Montreal\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m __contact__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRazvan Pascanu <r.pascanu@gmail>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtheano\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscan_module\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m scan_opt\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtheano\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscan_module\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscan\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m scan\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtheano\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscan_module\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscan_checkpoints\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m scan_checkpoints\n",
      "File \u001b[1;32md:\\Soft\\anaconda3\\envs\\aijc\\lib\\site-packages\\theano\\scan_module\\scan_opt.py:60\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtheano\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtheano\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor, scalar\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtheano\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m opt, get_scalar_constant_value, Alloc, AllocEmpty\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtheano\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gof\n",
      "File \u001b[1;32md:\\Soft\\anaconda3\\envs\\aijc\\lib\\site-packages\\theano\\tensor\\__init__.py:8\u001b[0m\n\u001b[0;32m      4\u001b[0m __docformat__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrestructuredtext en\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtheano\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbasic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtheano\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msubtensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtheano\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtype_other\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32md:\\Soft\\anaconda3\\envs\\aijc\\lib\\site-packages\\theano\\tensor\\basic.py:20\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtheano\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgof\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Apply, Constant, Op, Variable, ParamsType\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtheano\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgof\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtype\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Generic\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtheano\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscalar\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m int32 \u001b[38;5;28;01mas\u001b[39;00m int32_t\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtheano\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m elemwise\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtheano\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvar\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (AsTensorError, TensorVariable,\n\u001b[0;32m     23\u001b[0m                                TensorConstant, TensorConstantSignature,\n\u001b[0;32m     24\u001b[0m                                _tensor_py_operators)\n",
      "File \u001b[1;32md:\\Soft\\anaconda3\\envs\\aijc\\lib\\site-packages\\theano\\scalar\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m absolute_import, print_function, division\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbasic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbasic_scipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32md:\\Soft\\anaconda3\\envs\\aijc\\lib\\site-packages\\theano\\scalar\\basic.py:2370\u001b[0m\n\u001b[0;32m   2367\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2368\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m s\n\u001b[1;32m-> 2370\u001b[0m convert_to_bool \u001b[38;5;241m=\u001b[39m \u001b[43mCast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconvert_to_bool\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2371\u001b[0m convert_to_int8 \u001b[38;5;241m=\u001b[39m Cast(int8, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconvert_to_int8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   2372\u001b[0m convert_to_int16 \u001b[38;5;241m=\u001b[39m Cast(int16, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconvert_to_int16\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Soft\\anaconda3\\envs\\aijc\\lib\\site-packages\\theano\\scalar\\basic.py:2323\u001b[0m, in \u001b[0;36mCast.__init__\u001b[1;34m(self, o_type, name)\u001b[0m\n\u001b[0;32m   2321\u001b[0m \u001b[38;5;28msuper\u001b[39m(Cast, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(specific_out(o_type), name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m   2322\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mo_type \u001b[38;5;241m=\u001b[39m o_type\n\u001b[1;32m-> 2323\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mo_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Soft\\anaconda3\\envs\\aijc\\lib\\site-packages\\numpy\\__init__.py:338\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    333\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[1;32m--> 338\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtesting\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'bool'.\n`np.bool` was a deprecated alias for the builtin `bool`. To avoid this error in existing code, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import codecs\n",
    "import re\n",
    "import operator\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.layers import Layer\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints\n",
    "import keras.optimizers as opt\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dense, Activation, Embedding, Input\n",
    "from keras.models import Model\n",
    "import theano.tensor as T   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обработка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Я ждал, когда пройду 80% курса, чтобы написать...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Пока что трудно для понимания, так как я абсол...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Очень интересный курс. 4 звезды из 5 поставила...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Понравилась структура курса, заострение вниман...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Неплохой курс для начала изучения С/С++. Лекто...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Reviews\n",
       "0  Я ждал, когда пройду 80% курса, чтобы написать...\n",
       "1  Пока что трудно для понимания, так как я абсол...\n",
       "2  Очень интересный курс. 4 звезды из 5 поставила...\n",
       "3  Понравилась структура курса, заострение вниман...\n",
       "4  Неплохой курс для начала изучения С/С++. Лекто..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train_reviews.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4104 entries, 0 to 4575\n",
      "Data columns (total 1 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Reviews  4104 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 64.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m num_regex \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m^[+-]?[0-9]+\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.?[0-9]*$\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "num_regex = re.compile('^[+-]?[0-9]+\\.?[0-9]*$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(token):\n",
    "    return bool(num_regex.match(token))\n",
    "\n",
    "def create_vocab(domain, maxlen=0, vocab_size=0):\n",
    "    assert domain in {'restaurant', 'beer'}\n",
    "    source = '../preprocessed_data/'+domain+'/train.txt'\n",
    "\n",
    "    total_words, unique_words = 0, 0\n",
    "    word_freqs = {}\n",
    "    top = 0\n",
    "\n",
    "    fin = codecs.open(source, 'r', 'utf-8')\n",
    "    for line in fin:\n",
    "        words = line.split()\n",
    "        if maxlen > 0 and len(words) > maxlen:\n",
    "            continue\n",
    "\n",
    "        for w in words:\n",
    "            if not is_number(w):\n",
    "                try:\n",
    "                    word_freqs[w] += 1\n",
    "                except KeyError:\n",
    "                    unique_words += 1\n",
    "                    word_freqs[w] = 1\n",
    "                total_words += 1\n",
    "\n",
    "    print(('   %i total words, %i unique words' % (total_words, unique_words)))\n",
    "    sorted_word_freqs = sorted(word_freqs.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "    vocab = {'<pad>':0, '<unk>':1, '<num>':2}\n",
    "    index = len(vocab)\n",
    "    for word, _ in sorted_word_freqs:\n",
    "        vocab[word] = index\n",
    "        index += 1\n",
    "        if vocab_size > 0 and index > vocab_size + 2:\n",
    "            break\n",
    "    if vocab_size > 0:\n",
    "        print(('  keep the top %i words' % vocab_size))\n",
    "\n",
    "    #Write (vocab, frequence) to a txt file\n",
    "    vocab_file = codecs.open('../preprocessed_data/%s/vocab' % domain, mode='w', encoding='utf8')\n",
    "    sorted_vocab = sorted(vocab.items(), key=operator.itemgetter(1))\n",
    "    for word, index in sorted_vocab:\n",
    "        if index < 3:\n",
    "            vocab_file.write(word+'\\t'+str(0)+'\\n')\n",
    "            continue\n",
    "        vocab_file.write(word+'\\t'+str(word_freqs[word])+'\\n')\n",
    "    vocab_file.close()\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(domain, phase, vocab, maxlen):    \n",
    "    source = '../preprocessed_data/'+domain+'/'+phase+'.txt'\n",
    "    num_hit, unk_hit, total = 0., 0., 0.\n",
    "    maxlen_x = 0\n",
    "    data_x = []\n",
    "    \n",
    "    fin = codecs.open(source, 'r', 'utf-8')\n",
    "    for line in fin:\n",
    "        words = line.strip().split()\n",
    "        if maxlen > 0 and len(words) > maxlen:\n",
    "            continue\n",
    "\n",
    "        indices = []\n",
    "        for word in words:\n",
    "            if is_number(word):\n",
    "                indices.append(vocab['<num>'])\n",
    "                num_hit += 1\n",
    "            elif word in vocab:\n",
    "                indices.append(vocab[word])\n",
    "            else:\n",
    "                indices.append(vocab['<unk>'])\n",
    "                unk_hit += 1\n",
    "            total += 1\n",
    "\n",
    "        data_x.append(indices)\n",
    "        if maxlen_x < len(indices):\n",
    "            maxlen_x = len(indices)\n",
    "\n",
    "    print('   <num> hit rate: %.2f%%, <unk> hit rate: %.2f%%' % (100*num_hit/total, 100*unk_hit/total))\n",
    "    return data_x, maxlen_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(domain, vocab_size=0, maxlen=0):\n",
    "    vocab = create_vocab(domain, maxlen, vocab_size)\n",
    "    train_x, train_maxlen = read_dataset(domain, 'train', vocab, maxlen)\n",
    "    test_x, test_maxlen = read_dataset(domain, 'test', vocab, maxlen)\n",
    "    maxlen = max(train_maxlen, test_maxlen)\n",
    "    return vocab, train_x, test_x, maxlen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение модели "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Слои"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        \"\"\"\n",
    "        Keras Layer that implements an Content Attention mechanism.\n",
    "        Supports Masking.\n",
    "        \"\"\"\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert type(input_shape) == list\n",
    "        assert len(input_shape) == 2\n",
    "\n",
    "        self.steps = input_shape[0][1]\n",
    "\n",
    "        self.W = self.add_weight((input_shape[0][-1], input_shape[1][-1]),\n",
    "                                    initializer=self.init,\n",
    "                                    name='{}_W'.format(self.name),\n",
    "                                    regularizer=self.W_regularizer,\n",
    "                                    constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((1,),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input_tensor, mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, input_tensor, mask=None):\n",
    "        x = input_tensor[0]\n",
    "        y = input_tensor[1]\n",
    "        mask = mask[0]\n",
    "\n",
    "        y = K.transpose(K.dot(self.W, K.transpose(y)))\n",
    "        y = K.expand_dims(y, dim=-2)\n",
    "        y = K.repeat_elements(y, self.steps, axis=1)\n",
    "        eij = K.sum(x*y, axis=-1)\n",
    "\n",
    "        if self.bias:\n",
    "            b = K.repeat_elements(self.b, self.steps, axis=0)\n",
    "            eij += b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        return a\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSum(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        super(WeightedSum, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, input_tensor, mask=None):\n",
    "        assert type(input_tensor) == list\n",
    "        assert type(mask) == list\n",
    "\n",
    "        x = input_tensor[0]\n",
    "        a = input_tensor[1]\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][-1])\n",
    "\n",
    "    def compute_mask(self, x, mask=None):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedAspectEmb(Layer):\n",
    "    def __init__(self, input_dim, output_dim,\n",
    "                 init='uniform', input_length=None,\n",
    "                 W_regularizer=None, activity_regularizer=None,\n",
    "                 W_constraint=None,\n",
    "                 weights=None, dropout=0., **kwargs):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.init = initializers.get(init)\n",
    "        self.input_length = input_length\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.activity_regularizer = regularizers.get(activity_regularizer)\n",
    "\n",
    "        if 0. < self.dropout < 1.:\n",
    "            self.uses_learning_phase = True\n",
    "        self.initial_weights = weights\n",
    "        kwargs['input_shape'] = (self.input_length,)\n",
    "        kwargs['input_dtype'] = K.floatx()\n",
    "        super(WeightedAspectEmb, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight((self.input_dim, self.output_dim),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, x, mask=None):\n",
    "        return None\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return K.dot(x, self.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Average(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        super(Average, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            mask = K.expand_dims(mask)\n",
    "            x = x * mask\n",
    "        return K.sum(x, axis=-2) / K.sum(mask, axis=-2)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape[0:-2]+input_shape[-1:]\n",
    "    \n",
    "    def compute_mask(self, x, mask=None):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxMargin(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MaxMargin, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, input_tensor, mask=None):\n",
    "        z_s = input_tensor[0] \n",
    "        z_n = input_tensor[1]\n",
    "        r_s = input_tensor[2]\n",
    "\n",
    "        z_s = z_s / K.cast(K.epsilon() + K.sqrt(K.sum(K.square(z_s), axis=-1, keepdims=True)), K.floatx())\n",
    "        z_n = z_n / K.cast(K.epsilon() + K.sqrt(K.sum(K.square(z_n), axis=-1, keepdims=True)), K.floatx())\n",
    "        r_s = r_s / K.cast(K.epsilon() + K.sqrt(K.sum(K.square(r_s), axis=-1, keepdims=True)), K.floatx())\n",
    "\n",
    "        steps = z_n.shape[1]\n",
    "\n",
    "        pos = K.sum(z_s*r_s, axis=-1, keepdims=True)\n",
    "        pos = K.repeat_elements(pos, steps, axis=-1)\n",
    "        r_s = K.expand_dims(r_s, dim=-2)\n",
    "        r_s = K.repeat_elements(r_s, steps, axis=1)\n",
    "        neg = K.sum(z_n*r_s, axis=-1)\n",
    "\n",
    "        loss = K.cast(K.sum(T.maximum(0., (1. - pos + neg)), axis=-1, keepdims=True), K.floatx())\n",
    "        return loss\n",
    "\n",
    "    def compute_mask(self, input_tensor, mask=None):\n",
    "        return None\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0][0], 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(maxlen, vocab):\n",
    "\n",
    "    def ortho_reg(weight_matrix):\n",
    "        ### orthogonal regularization for aspect embedding matrix ###\n",
    "        w_n = weight_matrix / K.cast(K.epsilon() + K.sqrt(K.sum(K.square(weight_matrix), axis=-1, keepdims=True)), K.floatx())\n",
    "        reg = K.sum(K.square(K.dot(w_n, K.transpose(w_n)) - K.eye(w_n.shape[0].eval())))\n",
    "        return 0.1*reg\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    ##### Inputs #####\n",
    "    sentence_input = Input(shape=(maxlen,), dtype='int32', name='sentence_input')\n",
    "    neg_input = Input(shape=(20, maxlen), dtype='int32', name='neg_input')\n",
    "\n",
    "    ##### Construct word embedding layer #####\n",
    "    word_emb = Embedding(vocab_size, 200, mask_zero=True, name='word_emb')\n",
    "\n",
    "    ##### Compute sentence representation #####\n",
    "    e_w = word_emb(sentence_input)\n",
    "    y_s = Average()(e_w)\n",
    "    att_weights = Attention(name='att_weights')([e_w, y_s])\n",
    "    z_s = WeightedSum()([e_w, att_weights])\n",
    "\n",
    "    ##### Compute representations of negative instances #####\n",
    "    e_neg = word_emb(neg_input)\n",
    "    z_n = Average()(e_neg)\n",
    "\n",
    "    ##### Reconstruction #####\n",
    "    p_t = Dense(14)(z_s)\n",
    "    p_t = Activation('softmax', name='p_t')(p_t)\n",
    "    r_s = WeightedAspectEmb(14, 200, name='aspect_emb',\n",
    "            W_regularizer=ortho_reg)(p_t)\n",
    "\n",
    "    ##### Loss #####\n",
    "    loss = MaxMargin(name='max_margin')([z_s, z_n, r_s])\n",
    "    model = Model(input=[sentence_input, neg_input], output=loss)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, train_x, test_x, overall_maxlen = dataset.get_data(args.domain, vocab_size=args.vocab_size, maxlen=args.maxlen)\n",
    "train_x = sequence.pad_sequences(train_x, maxlen=overall_maxlen)\n",
    "test_x = sequence.pad_sequences(test_x, maxlen=overall_maxlen)\n",
    "\n",
    "print('Number of training examples: ', len(train_x))\n",
    "print('Length of vocab: ', len(vocab))\n",
    "\n",
    "def sentence_batch_generator(data, batch_size):\n",
    "    n_batch = len(data) / batch_size\n",
    "    batch_count = 0\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    while True:\n",
    "        if batch_count == n_batch:\n",
    "            np.random.shuffle(data)\n",
    "            batch_count = 0\n",
    "\n",
    "        batch = data[batch_count*batch_size: (batch_count+1)*batch_size]\n",
    "        batch_count += 1\n",
    "        yield batch\n",
    "\n",
    "def negative_batch_generator(data, batch_size, neg_size):\n",
    "    data_len = data.shape[0]\n",
    "    dim = data.shape[1]\n",
    "\n",
    "    while True:\n",
    "        indices = np.random.choice(data_len, batch_size * neg_size)\n",
    "        samples = data[indices].reshape(batch_size, neg_size, dim)\n",
    "        yield samples\n",
    "\n",
    "optimizer = opt.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, clipnorm=10, clipvalue=0)\n",
    "\n",
    "###############################################################################################################################\n",
    "## Building model\n",
    "\n",
    "def max_margin_loss(y_true, y_pred):\n",
    "    return K.mean(y_pred)\n",
    "\n",
    "model = create_model(overall_maxlen, vocab)\n",
    "# freeze the word embedding layer\n",
    "model.get_layer('word_emb').trainable=False\n",
    "model.compile(optimizer=optimizer, loss=max_margin_loss, metrics=[max_margin_loss])\n",
    "\n",
    "\n",
    "###############################################################################################################################\n",
    "## Training\n",
    "#\n",
    "\n",
    "vocab_inv = {}\n",
    "for w, ind in vocab.items():\n",
    "    vocab_inv[ind] = w\n",
    "\n",
    "\n",
    "sen_gen = sentence_batch_generator(train_x, 50)\n",
    "neg_gen = negative_batch_generator(train_x, 50, 20)\n",
    "# batches_per_epoch = len(train_x) / args.batch_size\n",
    "batches_per_epoch = 1000\n",
    "\n",
    "min_loss = float('inf')\n",
    "for ii in range(15):\n",
    "    t0 = time()\n",
    "    loss, max_margin_loss = 0., 0.\n",
    "\n",
    "    for b in tqdm(range(batches_per_epoch)):\n",
    "        sen_input = sen_gen.next()\n",
    "        neg_input = neg_gen.next()\n",
    "\n",
    "        batch_loss, batch_max_margin_loss = model.train_on_batch([sen_input, neg_input], np.ones((args.batch_size, 1)))\n",
    "        loss += batch_loss / batches_per_epoch\n",
    "        max_margin_loss += batch_max_margin_loss / batches_per_epoch\n",
    "\n",
    "    tr_time = time() - t0\n",
    "\n",
    "    if loss < min_loss:\n",
    "        min_loss = loss\n",
    "        word_emb = model.get_layer('word_emb').W.get_value()\n",
    "        aspect_emb = model.get_layer('aspect_emb').W.get_value()\n",
    "        word_emb = word_emb / np.linalg.norm(word_emb, axis=-1, keepdims=True)\n",
    "        aspect_emb = aspect_emb / np.linalg.norm(aspect_emb, axis=-1, keepdims=True)\n",
    "        aspect_file = codecs.open('/aspect.log', 'w', 'utf-8')\n",
    "        model.save_weights('/model_param')\n",
    "\n",
    "        for ind in range(len(aspect_emb)):\n",
    "            desc = aspect_emb[ind]\n",
    "            sims = word_emb.dot(desc.T)\n",
    "            ordered_words = np.argsort(sims)[::-1]\n",
    "            desc_list = [vocab_inv[w] for w in ordered_words[:100]]\n",
    "            print('Aspect %d:' % ind)\n",
    "            print(desc_list)\n",
    "            aspect_file.write('Aspect %d:\\n' % ind)\n",
    "            aspect_file.write(' '.join(desc_list) + '\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
