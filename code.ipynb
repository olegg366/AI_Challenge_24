{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i99BekPyjB-I"
      },
      "source": [
        "#### Для Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFlooW6yjGh3",
        "outputId": "bebd92b1-f710-4cec-bced-5b7c03f0f113"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZcSU3BEjL0V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dabb8fca-1a7e-4bfa-f79c-82c93e841a7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.12.1-py3-none-any.whl (431 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/431.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m399.4/431.4 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.4/431.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.10/dist-packages (from emoji) (4.12.2)\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.12.1\n"
          ]
        }
      ],
      "source": [
        "%pip install emoji"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQswnun_irbF"
      },
      "source": [
        "#### Подключение библиотек"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuRYn8TBirbJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from time import time\n",
        "from tqdm import tqdm\n",
        "from gensim.models import Word2Vec\n",
        "from emoji import is_emoji\n",
        "from string import punctuation\n",
        "from multiprocessing import cpu_count\n",
        "from functools import lru_cache\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.parameter import Parameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mh1LJhECirbK"
      },
      "source": [
        "## Обработка датасета"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "m3X1K-9iirbK",
        "outputId": "5b324515-6852-4f0b-96d5-06c036cddcfb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             Reviews\n",
              "0  Я ждал, когда пройду 80% курса, чтобы написать...\n",
              "1  Пока что трудно для понимания, так как я абсол...\n",
              "2  Очень интересный курс. 4 звезды из 5 поставила...\n",
              "3  Понравилась структура курса, заострение вниман...\n",
              "4  Неплохой курс для начала изучения С/С++. Лекто..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f7fb7c79-1695-4302-bbfd-93d7869bcbe3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Я ждал, когда пройду 80% курса, чтобы написать...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Пока что трудно для понимания, так как я абсол...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Очень интересный курс. 4 звезды из 5 поставила...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Понравилась структура курса, заострение вниман...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Неплохой курс для начала изучения С/С++. Лекто...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f7fb7c79-1695-4302-bbfd-93d7869bcbe3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f7fb7c79-1695-4302-bbfd-93d7869bcbe3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f7fb7c79-1695-4302-bbfd-93d7869bcbe3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c89f84cc-f74b-400e-ad51-9c15d87802db\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c89f84cc-f74b-400e-ad51-9c15d87802db')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c89f84cc-f74b-400e-ad51-9c15d87802db button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 4576,\n  \"fields\": [\n    {\n      \"column\": \"Reviews\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4104,\n        \"samples\": [\n          \"\\u041e\\u0442\\u043b\\u0438\\u0447\\u043d\\u044b\\u0439 \\u043a\\u0443\\u0440\\u0441, \\u0434\\u043b\\u044f \\u0436\\u0435\\u043b\\u0430\\u044e\\u0449\\u0438\\u0445 \\u043d\\u0430\\u0447\\u0430\\u0442\\u044c \\u0438\\u0437\\u0443\\u0447\\u0430\\u0442\\u044c DS - \\u0441\\u0430\\u043c\\u043e\\u0435 \\u0442\\u043e.\",\n          \"\\u0418\\u0437 \\u043f\\u0440\\u043e\\u0431\\u043b\\u0435\\u043c - \\u043e\\u043f\\u0435\\u0447\\u0430\\u0442\\u043a\\u0438. \\u0418\\u043d\\u043e\\u0433\\u0434\\u0430 \\u0434\\u043e\\u0432\\u043e\\u043b\\u044c\\u043d\\u043e \\u0441\\u0435\\u0440\\u044c\\u0435\\u0437\\u043d\\u044b\\u0435. \\u0415\\u0441\\u0442\\u044c \\u0432\\u043e\\u043f\\u0440\\u043e\\u0441\\u044b \\u043a \\u043f\\u043e\\u0440\\u044f\\u0434\\u043a\\u0443 \\u0441\\u043b\\u0435\\u0434\\u043e\\u0432\\u0430\\u043d\\u0438\\u044f \\u0442\\u0435\\u043c (\\u0442\\u0435\\u043e\\u0440\\u0438\\u044f/\\u0437\\u0430\\u0434\\u0430\\u0447\\u0438). \\u0414\\u043b\\u044f \\u043f\\u0435\\u0440\\u0432\\u043e\\u0433\\u043e \\u0437\\u043d\\u0430\\u043a\\u043e\\u043c\\u0441\\u0442\\u0432\\u0430 \\u0432\\u0440\\u044f\\u0434 \\u043b\\u0438 \\u0433\\u043e\\u0434\\u0438\\u0442\\u0441\\u044f. \\u041d\\u043e \\u0435\\u0441\\u043b\\u0438 \\u0432\\u0441\\u043f\\u043e\\u043c\\u043d\\u0438\\u0442\\u044c/\\u043f\\u043e\\u0442\\u0440\\u0435\\u043d\\u0438\\u0440\\u043e\\u0432\\u0430\\u0442\\u044c\\u0441\\u044f - \\u0437\\u0430\\u0434\\u0430\\u0447\\u043a\\u0438 \\u043e\\u0447\\u0435\\u043d\\u044c \\u0434\\u0430\\u0436\\u0435 \\u043d\\u0438\\u0447\\u0435\\u0433\\u043e. \\u041e\\u0441\\u043e\\u0431\\u0435\\u043d\\u043d\\u043e, \\u0435\\u0441\\u043b\\u0438 \\u0431\\u0435\\u0437 \\u043b\\u0438\\u0441\\u0442\\u043e\\u0447\\u043a\\u0430 \\u0438 \\u043a\\u0430\\u043b\\u044c\\u043a\\u0443\\u043b\\u044f\\u0442\\u043e\\u0440\\u0430.\",\n          \"\\u0415\\u0434\\u0438\\u043d\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u044b\\u0439 \\u043a\\u0443\\u0440\\u0441, \\u043a\\u043e\\u0442\\u043e\\u0440\\u044b\\u0439 \\u043f\\u043e\\u043c\\u043e\\u0433\\u0430\\u0435\\u0442 \\u043f\\u043e\\u0434\\u043d\\u044f\\u0442\\u044c\\u0441\\u044f \\u0441 \\u0431\\u0430\\u0437\\u043e\\u0432\\u043e\\u0433\\u043e \\u0443\\u0440\\u043e\\u0432\\u043d\\u044f \\u0434\\u043e \\u0441\\u043e\\u0434\\u0435\\u0440\\u0436\\u0430\\u0442\\u0435\\u043b\\u044c\\u043d\\u044b\\u0445 \\u0432\\u0435\\u0449\\u0435\\u0439.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df = pd.read_csv('train_reviews.csv', index_col=0)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUSYgWOT2LfR"
      },
      "outputs": [],
      "source": [
        "df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRqqYm-WirbM"
      },
      "outputs": [],
      "source": [
        "def rem_emoji(s: str):\n",
        "    i = 0\n",
        "    while i < len(s):\n",
        "        if is_emoji(s[i]):\n",
        "            s = s[:i] + s[i + 1:]\n",
        "        else: i += 1\n",
        "    if not s: return np.nan\n",
        "    return s\n",
        "\n",
        "df = df['Reviews'].apply(rem_emoji)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVSH-oszirbL"
      },
      "outputs": [],
      "source": [
        "df.dropna(inplace=True)\n",
        "df.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23cNSdqM2LfS"
      },
      "outputs": [],
      "source": [
        "tokenizer = TweetTokenizer(preserve_case=False)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stops = set(stopwords.words(\"russian\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6xlKTJHirbM"
      },
      "outputs": [],
      "source": [
        "@lru_cache(maxsize=1000000000)\n",
        "def lemmatize(w: str):\n",
        "    # caching the word-based lemmatizer to speed the process up\n",
        "    return lemmatizer.lemmatize(w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mV4WQda0irbM",
        "outputId": "2fbc445d-1bd1-4af8-ed30-a4bac375402d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4091/4091 [00:06<00:00, 637.36it/s] \n"
          ]
        }
      ],
      "source": [
        "newdf = []\n",
        "\n",
        "for text in tqdm(df):\n",
        "    # splitting into sentences\n",
        "    sentences = sent_tokenize(text)\n",
        "    tokenized_sentences = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "    # removing stopwords and non-alphanumeric tokens\n",
        "    lemmatized_sentences = [[lemmatize(word) for word in s if not word in stops and str.isalpha(word)]\n",
        "                            for s in tokenized_sentences]\n",
        "\n",
        "    for sentence in lemmatized_sentences:\n",
        "        newdf.append(\" \".join(sentence))\n",
        "df = newdf.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0dvq2o7irbN"
      },
      "outputs": [],
      "source": [
        "def read_data_batches(data, batch_size: int=50, minlength: int=5):\n",
        "    batch = []\n",
        "\n",
        "    for line in data:\n",
        "        line = line.strip().split()\n",
        "\n",
        "        if len(line) >= minlength:\n",
        "            batch.append(line)\n",
        "            if len(batch) >= batch_size:\n",
        "                yield batch\n",
        "                batch = []\n",
        "\n",
        "    if len(batch) > 0:\n",
        "        yield batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RkgVs0jirbN"
      },
      "outputs": [],
      "source": [
        "def text2vectors(text: list, w2v_model, maxlen: int, vocabulary):\n",
        "    acc_vecs = []\n",
        "\n",
        "    for word in text:\n",
        "        if word in w2v_model.wv and (vocabulary is None or word in vocabulary):\n",
        "            acc_vecs.append(w2v_model.wv[word])\n",
        "\n",
        "    if len(acc_vecs) < maxlen:\n",
        "        acc_vecs.extend([np.zeros(w2v_model.vector_size)] * (maxlen - len(acc_vecs)))\n",
        "\n",
        "    return acc_vecs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhk9FHtFirbO"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "for sentence in df:\n",
        "    data.append(sentence.split())\n",
        "\n",
        "w2v_model = Word2Vec(data, vector_size=200, window=10, min_count=0, workers=cpu_count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-jAWiZ_irbO"
      },
      "outputs": [],
      "source": [
        "def read_data_tensors(series, w2v_model,\n",
        "                      batch_size=50, vocabulary=None,\n",
        "                      maxlen=100, min_sent_length=5):\n",
        "\n",
        "    for batch in read_data_batches(series, batch_size, min_sent_length):\n",
        "        batch_vecs = []\n",
        "        batch_texts = []\n",
        "\n",
        "        for text in batch:\n",
        "            vectors_as_list = text2vectors(text, w2v_model, maxlen, vocabulary)\n",
        "            batch_vecs.append(np.asarray(vectors_as_list[:maxlen], dtype=np.float32))\n",
        "            batch_texts.append(text)\n",
        "\n",
        "        yield np.stack(batch_vecs, axis=0), batch_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycFeF3KW2LfT"
      },
      "outputs": [],
      "source": [
        "def vector2text(w2v_model: Word2Vec, vector):\n",
        "    return ' '.join([w2v_model.wv.most_similar(positive=i)[0][0] for i in vector])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iaLdfT51irbO"
      },
      "outputs": [],
      "source": [
        "def get_centroids(w2v_model, aspects_count):\n",
        "    \"\"\"\n",
        "        Clustering all word vectors with K-means and returning L2-normalizes\n",
        "        cluster centroids; used for ABAE aspects matrix initialization\n",
        "    \"\"\"\n",
        "\n",
        "    km = MiniBatchKMeans(n_clusters=aspects_count, verbose=0, n_init=100)\n",
        "    m = []\n",
        "\n",
        "    for k in w2v_model.wv.key_to_index:\n",
        "        m.append(w2v_model.wv[k])\n",
        "\n",
        "    m = np.array(m)\n",
        "    km.fit(m)\n",
        "    clusters = km.cluster_centers_\n",
        "\n",
        "    # L2 normalization\n",
        "    norm_aspect_matrix = clusters / np.linalg.norm(clusters, axis=-1, keepdims=True)\n",
        "\n",
        "    return norm_aspect_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G33jCmk9irbO"
      },
      "source": [
        "## Построение модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxe_5ovVirbO"
      },
      "source": [
        "### Слои"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mn1Tx9TDirbO"
      },
      "outputs": [],
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, wv_dim: int, maxlen: int):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.wv_dim = wv_dim\n",
        "\n",
        "        # max sentence length -- batch 2nd dim size\n",
        "        self.maxlen = maxlen\n",
        "        self.M = Parameter(torch.empty(size=(wv_dim, wv_dim)))\n",
        "        nn.init.kaiming_uniform_(self.M.data)\n",
        "\n",
        "        # softmax for attending to wod vectors\n",
        "        self.attention_softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, input_embeddings):\n",
        "        # (b, wv, 1)\n",
        "        mean_embedding = torch.mean(input_embeddings, (1,)).unsqueeze(2)\n",
        "\n",
        "        # (wv, wv) x (b, wv, 1) -> (b, wv, 1)\n",
        "        product_1 = torch.matmul(self.M, mean_embedding)\n",
        "\n",
        "        # (b, maxlen, wv) x (b, wv, 1) -> (b, maxlen, 1)\n",
        "        product_2 = torch.matmul(input_embeddings, product_1).squeeze(2)\n",
        "\n",
        "        results = self.attention_softmax(product_2)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def extra_repr(self):\n",
        "        return 'wv_dim={}, maxlen={}'.format(self.wv_dim, self.maxlen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5dLWmd7irbP"
      },
      "outputs": [],
      "source": [
        "class ABAE(nn.Module):\n",
        "    \"\"\"\n",
        "        The model described in the paper ``An Unsupervised Neural Attention Model for Aspect Extraction''\n",
        "        by He, Ruidan and  Lee, Wee Sun  and  Ng, Hwee Tou  and  Dahlmeier, Daniel, ACL2017\n",
        "        https://aclweb.org/anthology/papers/P/P17/P17-1036/\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, wv_dim: int = 200, asp_count: int = 5,\n",
        "                 ortho_reg: float = 0.1, maxlen: int = 201, init_aspects_matrix=None):\n",
        "        \"\"\"\n",
        "        Initializing the model\n",
        "\n",
        "        :param wv_dim: word vector size\n",
        "        :param asp_count: number of aspects\n",
        "        :param ortho_reg: coefficient for tuning the ortho-regularizer's influence\n",
        "        :param maxlen: sentence max length taken into account\n",
        "        :param init_aspects_matrix: None or init. matrix for aspects\n",
        "        \"\"\"\n",
        "        super(ABAE, self).__init__()\n",
        "        self.wv_dim = wv_dim\n",
        "        self.asp_count = asp_count\n",
        "        self.ortho = ortho_reg\n",
        "        self.maxlen = maxlen\n",
        "\n",
        "        self.attention = SelfAttention(wv_dim, maxlen)\n",
        "        self.linear_transform = nn.Linear(self.wv_dim, self.asp_count)\n",
        "        self.softmax_aspects = nn.Softmax(dim=-1)\n",
        "        self.aspects_embeddings = Parameter(torch.empty(size=(wv_dim, asp_count)))\n",
        "\n",
        "        if init_aspects_matrix is None:\n",
        "            nn.init.xavier_uniform(self.aspects_embeddings)\n",
        "        else:\n",
        "            self.aspects_embeddings.data = torch.from_numpy(init_aspects_matrix.T).to('cuda')\n",
        "\n",
        "    def get_aspects_importances(self, text_embeddings):\n",
        "        \"\"\"\n",
        "            Takes embeddings of a sentence as input, returns attention weights\n",
        "        \"\"\"\n",
        "\n",
        "        # compute attention scores, looking at text embeddings average\n",
        "        attention_weights = self.attention(text_embeddings)\n",
        "\n",
        "        # multiplying text embeddings by attention scores -- and summing\n",
        "        # (matmul: we sum every word embedding's coordinate with attention weights)\n",
        "        weighted_text_emb = torch.matmul(attention_weights.unsqueeze(1),  # (batch, 1, sentence)\n",
        "                                         text_embeddings  # (batch, sentence, wv_dim)\n",
        "                                         ).squeeze()\n",
        "\n",
        "        # encoding with a simple feed-forward layer (wv_dim) -> (aspects_count)\n",
        "        raw_importances = self.linear_transform(weighted_text_emb)\n",
        "\n",
        "        # computing 'aspects distribution in a sentence'\n",
        "        aspects_importances = self.softmax_aspects(raw_importances)\n",
        "\n",
        "        return attention_weights, aspects_importances, weighted_text_emb\n",
        "\n",
        "    def forward(self, text_embeddings, negative_samples_texts):\n",
        "        # negative samples are averaged\n",
        "        averaged_negative_samples = torch.mean(negative_samples_texts, dim=2)\n",
        "\n",
        "        # encoding: words embeddings -> sentence embedding, aspects importances\n",
        "        _, aspects_importances, weighted_text_emb = self.get_aspects_importances(text_embeddings)\n",
        "\n",
        "        # decoding: aspects embeddings matrix, aspects_importances -> recovered sentence embedding\n",
        "        recovered_emb = torch.matmul(self.aspects_embeddings, aspects_importances.unsqueeze(2)).squeeze()\n",
        "\n",
        "        # loss\n",
        "        reconstruction_triplet_loss = ABAE._reconstruction_loss(weighted_text_emb,\n",
        "                                                                recovered_emb,\n",
        "                                                                averaged_negative_samples)\n",
        "        max_margin = torch \\\n",
        "            .max(reconstruction_triplet_loss, torch.zeros_like(reconstruction_triplet_loss)) \\\n",
        "            .unsqueeze(dim=-1)\n",
        "\n",
        "        return self.ortho * self._ortho_regularizer() + max_margin\n",
        "\n",
        "    @staticmethod\n",
        "    def _reconstruction_loss(text_emb, recovered_emb, averaged_negative_emb):\n",
        "\n",
        "        positive_dot_products = torch.matmul(text_emb.unsqueeze(1), recovered_emb.unsqueeze(2)).squeeze()\n",
        "        negative_dot_products = torch.matmul(averaged_negative_emb, recovered_emb.unsqueeze(2)).squeeze()\n",
        "        reconstruction_triplet_loss = torch.sum(1 - positive_dot_products.unsqueeze(1), dim=1)\n",
        "\n",
        "        return reconstruction_triplet_loss\n",
        "\n",
        "    def _ortho_regularizer(self):\n",
        "        return torch.norm(\n",
        "            torch.matmul(self.aspects_embeddings.t(), self.aspects_embeddings) \\\n",
        "            - torch.eye(self.asp_count).to('cuda'))\n",
        "\n",
        "    def get_aspect_words(self, w2v_model: Word2Vec, topn=15):\n",
        "        words = []\n",
        "\n",
        "        # getting aspects embeddings\n",
        "        aspects = self.aspects_embeddings.cpu().detach().numpy().T\n",
        "\n",
        "        # getting scalar products of word embeddings and aspect embeddings;\n",
        "        # to obtain the ``probabilities'', one should also apply softmax\n",
        "        # words_scores = w2v_model.wv.syn0.dot(aspects)\n",
        "        # words_scores = w2v_model.wv.vectors.dot(aspects)\n",
        "\n",
        "        # for row in range(aspects.shape[1]):\n",
        "        #     argmax_scalar_products = np.argsort(-words_scores[:, row])[:topn]\n",
        "        #     # print([w for w, dist in w2v_model.wv.similar_by_vector(aspects.T[row])[:topn]])\n",
        "        #     words.append([w2v_model.wv.index_to_key[i] for i in argmax_scalar_products])\n",
        "\n",
        "        for aspect in aspects:\n",
        "            words.append(w2v_model.wv.most_similar(aspect))\n",
        "\n",
        "        return words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5Sb1sK3irbP"
      },
      "source": [
        "### Обучение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUwC7G9TirbP"
      },
      "outputs": [],
      "source": [
        "batch_size = 50\n",
        "epochs = 100\n",
        "max_len = max([len(s.split()) for s in df])\n",
        "neg = 20\n",
        "log_progress_steps = 1\n",
        "aspects_number = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_p6p6SCTirbP",
        "outputId": "e2c2d508-1f41-4647-bd3a-901e1c9db073"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "[1] понимаю ответа одной статистику ещё комментариев лекции которая проходил начал\n",
            "[2] комментариев ответа вывод удовольствие одной интересный понимаю изложение тестов статистику\n",
            "[3] прикладной бусти вывод куски силы тимур комьюнити статьи полученных ответить\n",
            "[4] достоен усвояемый огненный молчу насыщенный добавите политика жалеть уурс невозможного\n",
            "[5] актуальность завершен re поздних подтянула дмитрию покороче ирина интереснейших темном\n",
            "Loss: 67362.5781\n",
            "Epoch 2/100\n",
            "[1] комментариев ответа понимаю статистику одной которая проходил удовольствие изложение желаю\n",
            "[2] комьюнити вступление начинающего собой науке случайных душ графиков искал понимает\n",
            "[3] обьясняют неизвестных интерфейсов устройства раскрытия порядку г задумываться каждое старания\n",
            "[4] усвояемый политика молчу уурс достоен оторванность жалеть приложенные усталость сверхподробный\n",
            "[5] актуальность re завершен поздних дмитрию интереснейших подтянула планку покороче повторишь\n",
            "Loss: 35014.0117\n",
            "Epoch 3/100\n",
            "[1] комментариев ответа вывод желаю понимаю идеально удовольствие изложение статистику одной\n",
            "[2] душ вступление учится продвижении знаниям марина смогут комьюнити приёмов благодарности\n",
            "[3] смешные надежные неизвестных затронута комкано устройства порядку приоритет коллективу f\n",
            "[4] усвояемый политика молчу оторванность уурс достоен жалеть сверхподробный усталость приложенные\n",
            "[5] актуальность re завершен поздних интереснейших подтянула дмитрию покороче невозможного планку\n",
            "Loss: 20792.1680\n",
            "Epoch 4/100\n",
            "[1] комментариев вывод ответа желаю идеально ценность единственный удовольствие жизни зрения\n",
            "[2] продвижении учится душ марина вступление капли приёмов воевать нарушения ограниченное\n",
            "[3] объявлять юным затронута смешные лайка надежные программировала тренируюсь экономические выбросы\n",
            "[4] усвояемый политика молчу оторванность уурс достоен жалеть сверхподробный усталость приложенные\n",
            "[5] актуальность re завершен поздних подтянула интереснейших дмитрию невозможного покороче планку\n",
            "Loss: 13425.1836\n",
            "Epoch 5/100\n",
            "[1] вывод комментариев ответа желаю ценность единственный показаться идеально зрения жизни\n",
            "[2] продвижении капли воевать учится марина нарушения проделанная душ репетитором приёмов\n",
            "[3] объявлять жемчужина душевное поразминать юным программировала машиной ведём лайка матану\n",
            "[4] усвояемый политика оторванность молчу уурс жалеть достоен усталость сверхподробный приложенные\n",
            "[5] актуальность re завершен подтянула поздних интереснейших дмитрию невозможного планку черт\n",
            "Loss: 9181.0586\n",
            "Epoch 6/100\n",
            "[1] вывод комментариев ценность ответа единственный желаю показаться полученных зрения идеально\n",
            "[2] молекулами продвижении воевать связывать капли тратит репетитором нарушения специализированной затрагиваться\n",
            "[3] объявлять жемчужина душевное оличный поразминать предшествующий подошли серьезные курсище ведём\n",
            "[4] усвояемый политика оторванность молчу уурс жалеть усталость достоен приложенные сверхподробный\n",
            "[5] актуальность re завершен подтянула поздних дмитрию интереснейших невозможного планку черт\n",
            "Loss: 6547.6299\n",
            "Epoch 7/100\n",
            "[1] вывод комментариев ценность единственный полученных ответа показаться желаю структура зрения\n",
            "[2] молекулами обновили тратит связывать логичным затрагиваться collection нынешнюю комплекте продвижении\n",
            "[3] объявлять жемчужина краткая оличный мегакрутой душевное предшествующий донесён подошли курсище\n",
            "[4] усвояемый политика оторванность молчу жалеть уурс усталость достоен приложенные костылями\n",
            "[5] актуальность re подтянула завершен поздних планку невозможного дмитрию интереснейших черт\n",
            "Loss: 4821.4097\n",
            "Epoch 8/100\n",
            "[1] вывод полученных ценность единственный комментариев показаться желаю ответа структура тимур\n",
            "[2] молекулами логичным скучать обновили collection тратит потные связывать натуральных затрагиваться\n",
            "[3] объявлять жемчужина краткая мегакрутой оличный предшествующий донесён крутейшие душевное легкоосваиваемый\n",
            "[4] усвояемый политика оторванность молчу жалеть уурс усталость приложенные достоен закончу\n",
            "[5] актуальность re подтянула поздних завершен планку невозможного черт дмитрию интереснейших\n",
            "Loss: 3641.7749\n",
            "Epoch 9/100\n",
            "[1] вывод полученных единственный ценность комментариев показаться желаю ответить ответа бусти\n",
            "[2] молекулами скучать потные логичным платформа обновили трудное collection решенных натуральных\n",
            "[3] объявлять краткая жемчужина мегакрутой крутейшие легкоосваиваемый оличный донесён лаконичный предшествующий\n",
            "[4] усвояемый политика оторванность молчу жалеть уурс усталость приложенные закончу достоен\n",
            "[5] актуальность re подтянула поздних завершен планку невозможного черт дмитрию интереснейших\n",
            "Loss: 2808.4912\n",
            "Epoch 10/100\n",
            "[1] вывод полученных единственный ценность показаться бусти комментариев ответить система желаю\n",
            "[2] скучать молекулами потные платформа логичным трудное обратите решенных обновили разнобой\n",
            "[3] краткая объявлять мегакрутой жемчужина крутейшие легкоосваиваемый лаконичный оличный позновательный сделанный\n",
            "[4] усвояемый политика оторванность молчу жалеть уурс усталость приложенные закончу достоен\n",
            "[5] актуальность re подтянула поздних завершен планку невозможного черт дмитрию интереснейших\n",
            "Loss: 2203.7207\n",
            "Epoch 11/100\n",
            "[1] вывод полученных единственный бусти показаться ценность система ответить переменные комментариев\n",
            "[2] скучать потные платформа молекулами обратите заинтересовало потугой логичным разнобой трудное\n",
            "[3] краткая объявлять мегакрутой крутейшие легкоосваиваемый жемчужина лаконичный позновательный сделанный третий\n",
            "[4] усвояемый политика оторванность молчу жалеть уурс усталость приложенные закончу достоен\n",
            "[5] актуальность re подтянула поздних завершен невозможного планку черт интереснейших дмитрию\n",
            "Loss: 1754.8242\n",
            "Epoch 12/100\n",
            "[1] вывод полученных бусти переменные система единственный ответить показаться ценность комментариев\n",
            "[2] заинтересовало потные скучать обратите платформа потугой кайфовый влюблена молекулами разнобой\n",
            "[3] краткая объявлять крутейшие мегакрутой легкоосваиваемый лаконичный позновательный жемчужина сделанный третий\n",
            "[4] усвояемый политика оторванность молчу жалеть уурс усталость приложенные достоен закончу\n",
            "[5] актуальность re подтянула поздних завершен невозможного черт планку интереснейших дмитрию\n",
            "Loss: 1415.2357\n",
            "Epoch 13/100\n",
            "[1] вывод полученных переменные бусти система ответить единственный показаться ценность комьюнити\n",
            "[2] заинтересовало кайфовый потные потугой обратите скучать влюблена третий платформа разнобой\n",
            "[3] краткая крутейшие объявлять мегакрутой легкоосваиваемый лаконичный позновательный сделанный жемчужина третий\n",
            "[4] усвояемый политика оторванность молчу жалеть уурс усталость приложенные достоен закончу\n",
            "[5] актуальность re подтянула поздних завершен невозможного черт планку интереснейших показать\n",
            "Loss: 1154.1206\n",
            "Epoch 14/100\n",
            "[1] вывод полученных переменные бусти система ответить единственный показаться комьюнити ценность\n",
            "[2] заинтересовало третий кайфовый влюблена потугой обратите структурного потные скучать платформа\n",
            "[3] краткая крутейшие легкоосваиваемый мегакрутой объявлять лаконичный позновательный сделанный третий жемчужина\n",
            "[4] усвояемый политика оторванность молчу жалеть уурс усталость приложенные достоен закончу\n",
            "[5] актуальность re подтянула поздних завершен невозможного черт планку интереснейших показать\n",
            "Loss: 950.4882\n",
            "Epoch 15/100\n",
            "[1] вывод переменные полученных бусти система ответить единственный комьюнити показаться прикладной\n",
            "[2] заинтересовало третий кайфовый структурного влюблена потугой невсегда обратите тетрадкой тяжелый\n",
            "[3] краткая крутейшие легкоосваиваемый мегакрутой лаконичный объявлять позновательный сделанный третий недоработана\n",
            "[4] усвояемый политика оторванность жалеть молчу уурс усталость приложенные достоен закончу\n",
            "[5] актуальность re подтянула поздних завершен черт невозможного планку показать интереснейших\n",
            "Loss: 789.7026\n",
            "Epoch 16/100\n",
            "[1] вывод переменные полученных система бусти ответить комьюнити единственный показаться прикладной\n",
            "[2] третий заинтересовало структурного кайфовый систематизация влюблена невсегда позновательный русскоязычный потугой\n",
            "[3] краткая крутейшие легкоосваиваемый мегакрутой лаконичный позновательный объявлять сделанный третий недоработана\n",
            "[4] усвояемый политика жалеть молчу оторванность уурс усталость приложенные достоен закончу\n",
            "[5] актуальность re подтянула поздних завершен черт невозможного планку показать интереснейших\n",
            "Loss: 661.3500\n",
            "Epoch 17/100\n",
            "[1] переменные вывод полученных система бусти ответить комьюнити единственный показаться прикладной\n",
            "[2] третий заинтересовало структурного позновательный систематизация невсегда кайфовый русскоязычный жаловаться влюблена\n",
            "[3] краткая крутейшие легкоосваиваемый лаконичный мегакрутой позновательный сделанный объявлять третий недоработана\n",
            "[4] усвояемый политика жалеть молчу оторванность уурс усталость достоен приложенные закончу\n",
            "[5] актуальность re подтянула поздних завершен черт невозможного планку показать интереснейших\n",
            "Loss: 557.8850\n",
            "Epoch 18/100\n",
            "[1] переменные вывод система полученных бусти ответить комьюнити единственный показаться прикладной\n",
            "[2] третий заинтересовало позновательный систематизация структурного невсегда русскоязычный жаловаться кайфовый влюблена\n",
            "[3] краткая крутейшие легкоосваиваемый лаконичный мегакрутой позновательный сделанный объявлять третий недоработана\n",
            "[4] усвояемый политика жалеть молчу оторванность уурс усталость достоен приложенные закончу\n",
            "[5] актуальность re подтянула поздних завершен черт невозможного показать планку интереснейших\n",
            "Loss: 473.7509\n",
            "Epoch 19/100\n",
            "[1] переменные вывод система бусти полученных ответить комьюнити единственный показаться прикладной\n",
            "[2] третий заинтересовало позновательный систематизация структурного русскоязычный невсегда жаловаться интенсивный калькулятор\n",
            "[3] краткая крутейшие легкоосваиваемый лаконичный позновательный мегакрутой сделанный объявлять недоработана третий\n",
            "[4] усвояемый политика жалеть молчу оторванность уурс усталость достоен приложенные закончу\n",
            "[5] актуальность re подтянула поздних завершен черт невозможного показать планку темном\n",
            "Loss: 404.7961\n",
            "Epoch 20/100\n",
            "[1] переменные вывод система бусти полученных ответить комьюнити единственный присутствуют показаться\n",
            "[2] третий позновательный заинтересовало систематизация структурного жаловаться русскоязычный невсегда интенсивный калькулятор\n",
            "[3] краткая крутейшие легкоосваиваемый лаконичный позновательный мегакрутой сделанный объявлять недоработана третий\n",
            "[4] политика усвояемый жалеть молчу оторванность уурс усталость приложенные достоен закончу\n",
            "[5] актуальность re подтянула поздних завершен черт невозможного показать планку темном\n",
            "Loss: 347.8775\n",
            "Epoch 21/100\n",
            "[1] переменные вывод система бусти полученных ответить комьюнити присутствуют единственный прикладной\n",
            "[2] третий позновательный систематизация заинтересовало структурного жаловаться русскоязычный интенсивный невсегда сделанный\n",
            "[3] краткая крутейшие легкоосваиваемый лаконичный позновательный мегакрутой сделанный объявлять недоработана третий\n",
            "[4] политика усвояемый жалеть молчу оторванность усталость уурс приложенные закончу достоен\n",
            "[5] актуальность re подтянула поздних завершен черт невозможного показать планку темном\n",
            "Loss: 300.5894\n",
            "Epoch 22/100\n",
            "[1] переменные система вывод бусти полученных ответить комьюнити присутствуют единственный прикладной\n",
            "[2] третий позновательный систематизация заинтересовало структурного интенсивный жаловаться русскоязычный невсегда сделанный\n",
            "[3] краткая крутейшие легкоосваиваемый лаконичный позновательный мегакрутой сделанный недоработана объявлять третий\n",
            "[4] политика усвояемый жалеть молчу оторванность усталость уурс закончу приложенные достоен\n",
            "[5] актуальность re подтянула поздних завершен черт невозможного показать темном планку\n",
            "Loss: 261.0698\n",
            "Epoch 23/100\n",
            "[1] переменные система вывод бусти ответить полученных комьюнити присутствуют упор прикладной\n",
            "[2] третий позновательный систематизация заинтересовало интенсивный жаловаться структурного русскоязычный сделанный лаконичный\n",
            "[3] краткая крутейшие легкоосваиваемый лаконичный позновательный мегакрутой сделанный недоработана объявлять интенсивный\n",
            "[4] политика усвояемый жалеть молчу оторванность усталость уурс закончу приложенные достоен\n",
            "[5] актуальность re подтянула поздних завершен черт невозможного показать темном планку\n",
            "Loss: 227.8640\n",
            "Epoch 24/100\n",
            "[1] переменные система вывод бусти ответить полученных комьюнити присутствуют упор прикладной\n",
            "[2] третий позновательный систематизация заинтересовало интенсивный жаловаться структурного сделанный лаконичный русскоязычный\n",
            "[3] краткая крутейшие легкоосваиваемый лаконичный позновательный мегакрутой сделанный недоработана интенсивный третий\n",
            "[4] политика усвояемый жалеть молчу оторванность усталость уурс закончу приложенные достоен\n",
            "[5] актуальность re подтянула поздних черт завершен невозможного показать темном планку\n",
            "Loss: 199.8276\n",
            "Epoch 25/100\n",
            "[1] переменные система вывод бусти ответить полученных комьюнити присутствуют упор прикладной\n",
            "[2] третий позновательный систематизация интенсивный заинтересовало сделанный жаловаться лаконичный структурного русскоязычный\n",
            "[3] краткая крутейшие легкоосваиваемый лаконичный позновательный мегакрутой сделанный недоработана интенсивный записана\n",
            "[4] политика усвояемый жалеть молчу оторванность усталость уурс закончу приложенные достоен\n",
            "[5] актуальность re подтянула поздних черт завершен показать невозможного темном планку\n",
            "Loss: 176.0576\n",
            "Epoch 26/100\n",
            "[1] переменные система вывод бусти ответить комьюнити полученных присутствуют упор прикладной\n",
            "[2] третий позновательный систематизация интенсивный заинтересовало сделанный лаконичный жаловаться тренажером записана\n",
            "[3] краткая крутейшие легкоосваиваемый лаконичный позновательный мегакрутой сделанный недоработана интенсивный записана\n",
            "[4] политика усвояемый жалеть молчу оторванность усталость уурс закончу приложенные достоен\n",
            "[5] актуальность re подтянула поздних черт показать невозможного завершен темном планку\n",
            "Loss: 155.8102\n",
            "Epoch 27/100\n",
            "[1] переменные система вывод бусти ответить комьюнити полученных присутствуют упор выше\n",
            "[2] позновательный третий систематизация интенсивный сделанный лаконичный заинтересовало жаловаться тренажером записана\n",
            "[3] краткая крутейшие легкоосваиваемый лаконичный позновательный сделанный мегакрутой недоработана интенсивный записана\n",
            "[4] политика усвояемый жалеть молчу оторванность усталость уурс закончу приложенные достоен\n",
            "[5] актуальность re подтянула поздних черт показать невозможного завершен темном планку\n",
            "Loss: 138.4978\n",
            "Epoch 28/100\n",
            "[1] переменные система вывод ответить бусти комьюнити полученных присутствуют упор выше\n",
            "[2] позновательный третий систематизация интенсивный сделанный лаконичный жаловаться заинтересовало тренажером записана\n",
            "[3] краткая крутейшие легкоосваиваемый лаконичный позновательный сделанный мегакрутой недоработана интенсивный записана\n",
            "[4] политика усвояемый жалеть молчу оторванность усталость уурс закончу приложенные добавите\n",
            "[5] актуальность re подтянула поздних черт показать невозможного темном завершен планку\n",
            "Loss: 123.6430\n",
            "Epoch 29/100\n",
            "[1] переменные система вывод ответить бусти комьюнити полученных присутствуют упор место\n",
            "[2] позновательный третий систематизация интенсивный сделанный лаконичный тренажером жаловаться записана заинтересовало\n",
            "[3] крутейшие краткая легкоосваиваемый лаконичный позновательный сделанный мегакрутой недоработана интенсивный записана\n",
            "[4] политика усвояемый жалеть молчу оторванность усталость уурс закончу приложенные добавите\n",
            "[5] актуальность re подтянула поздних черт показать невозможного темном завершен планку\n",
            "Loss: 110.8546\n",
            "Epoch 30/100\n",
            "[1] переменные система вывод ответить бусти комьюнити присутствуют полученных упор место\n",
            "[2] позновательный третий систематизация интенсивный сделанный лаконичный тренажером записана жаловаться заинтересовало\n",
            "[3] крутейшие краткая легкоосваиваемый лаконичный позновательный сделанный мегакрутой недоработана интенсивный записана\n",
            "[4] политика усвояемый жалеть молчу оторванность усталость уурс закончу приложенные добавите\n",
            "[5] актуальность re подтянула поздних черт показать невозможного темном планку завершен\n",
            "Loss: 99.8130\n",
            "Epoch 31/100\n",
            "[1] переменные система вывод ответить бусти комьюнити присутствуют упор полученных место\n",
            "[2] позновательный третий систематизация интенсивный сделанный лаконичный тренажером записана жаловаться заинтересовало\n",
            "[3] крутейшие краткая легкоосваиваемый лаконичный позновательный сделанный мегакрутой недоработана интенсивный записана\n",
            "[4] политика усвояемый жалеть молчу оторванность усталость уурс закончу приложенные доски\n",
            "[5] актуальность re подтянула поздних показать черт невозможного темном планку рассказаны\n",
            "Loss: 90.2529\n",
            "Epoch 32/100\n",
            "[1] переменные система вывод ответить бусти присутствуют комьюнити упор полученных место\n",
            "[2] позновательный третий систематизация интенсивный сделанный лаконичный тренажером записана жаловаться легкоосваиваемый\n",
            "[3] крутейшие краткая легкоосваиваемый лаконичный позновательный сделанный мегакрутой недоработана интенсивный записана\n",
            "[4] политика усвояемый жалеть молчу оторванность усталость уурс закончу приложенные доски\n",
            "[5] актуальность re подтянула поздних показать черт невозможного темном планку рассказаны\n",
            "Loss: 81.9546\n",
            "Epoch 33/100\n",
            "[1] переменные система ответить вывод бусти присутствуют упор комьюнити полученных место\n",
            "[2] позновательный третий систематизация интенсивный сделанный лаконичный тренажером записана жаловаться легкоосваиваемый\n",
            "[3] крутейшие краткая легкоосваиваемый лаконичный позновательный сделанный мегакрутой недоработана интенсивный записана\n",
            "[4] политика жалеть усвояемый молчу оторванность усталость уурс закончу приложенные доски\n",
            "[5] актуальность re подтянула поздних показать черт невозможного темном рассказаны планку\n",
            "Loss: 74.7350\n",
            "Epoch 34/100\n",
            "[1] переменные система ответить вывод бусти упор присутствуют комьюнити полученных место\n",
            "[2] позновательный третий систематизация интенсивный сделанный лаконичный тренажером записана жаловаться легкоосваиваемый\n",
            "[3] крутейшие краткая легкоосваиваемый лаконичный позновательный сделанный мегакрутой недоработана интенсивный записана\n",
            "[4] политика жалеть усвояемый молчу оторванность усталость закончу уурс приложенные доски\n",
            "[5] актуальность re подтянула поздних показать черт невозможного темном рассказаны планку\n",
            "Loss: 68.4406\n",
            "Epoch 35/100\n",
            "[1] переменные система ответить вывод упор бусти присутствуют комьюнити полученных место\n",
            "[2] позновательный третий систематизация интенсивный сделанный лаконичный тренажером записана жаловаться легкоосваиваемый\n",
            "[3] крутейшие краткая легкоосваиваемый лаконичный позновательный сделанный мегакрутой недоработана интенсивный записана\n",
            "[4] политика жалеть усвояемый молчу оторванность усталость закончу уурс приложенные доски\n",
            "[5] актуальность re подтянула поздних показать черт темном невозможного рассказаны заметками\n",
            "Loss: 62.9422\n",
            "Epoch 36/100\n",
            "[1] переменные система ответить вывод упор бусти присутствуют комьюнити полученных место\n",
            "[2] позновательный третий систематизация интенсивный сделанный лаконичный тренажером записана жаловаться легкоосваиваемый\n",
            "[3] крутейшие краткая легкоосваиваемый лаконичный позновательный сделанный мегакрутой недоработана интенсивный записана\n",
            "[4] политика жалеть усвояемый молчу оторванность усталость закончу уурс приложенные доски\n",
            "[5] актуальность re подтянула поздних показать черт темном невозможного заметками рассказаны\n",
            "Loss: 58.1309\n",
            "Epoch 37/100\n",
            "[1] переменные система ответить вывод упор присутствуют бусти комьюнити полученных место\n",
            "[2] позновательный третий систематизация интенсивный сделанный лаконичный тренажером записана легкоосваиваемый жаловаться\n",
            "[3] крутейшие краткая легкоосваиваемый лаконичный позновательный сделанный мегакрутой недоработана интенсивный записана\n",
            "[4] политика жалеть молчу усвояемый оторванность усталость закончу уурс приложенные доски\n",
            "[5] актуальность re подтянула поздних показать черт темном невозможного заметками рассказаны\n",
            "Loss: 53.9140\n",
            "Epoch 38/100\n",
            "[1] переменные система ответить упор вывод присутствуют бусти комьюнити место полученных\n",
            "[2] позновательный третий систематизация интенсивный сделанный лаконичный тренажером записана легкоосваиваемый жаловаться\n",
            "[3] крутейшие краткая легкоосваиваемый лаконичный позновательный сделанный мегакрутой недоработана интенсивный записана\n",
            "[4] политика жалеть молчу усвояемый оторванность усталость закончу уурс приложенные доски\n",
            "[5] актуальность re подтянула поздних показать черт темном невозможного заметками повторишь\n",
            "Loss: 50.2134\n",
            "Epoch 39/100\n",
            "[1] переменные система ответить упор вывод присутствуют бусти комьюнити место полученных\n",
            "[2] позновательный третий систематизация интенсивный сделанный лаконичный тренажером записана легкоосваиваемый жаловаться\n",
            "[3] крутейшие краткая легкоосваиваемый лаконичный позновательный сделанный мегакрутой недоработана интенсивный записана\n",
            "[4] политика жалеть молчу усвояемый оторванность усталость закончу уурс доски приложенные\n",
            "[5] актуальность re подтянула поздних показать черт темном заметками невозможного повторишь\n",
            "Loss: 46.9619\n",
            "Epoch 40/100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-ad88460315f9>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mitem_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pad with 0 if smaller than batch size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-412fd1fd9906>\u001b[0m in \u001b[0;36mread_data_tensors\u001b[0;34m(series, w2v_model, batch_size, vocabulary, maxlen, min_sent_length)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mvectors_as_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext2vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mbatch_vecs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors_as_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mbatch_texts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "wv_dim = w2v_model.vector_size\n",
        "y = torch.zeros((batch_size, 1)).to('cuda')\n",
        "\n",
        "model = ABAE(wv_dim=wv_dim,\n",
        "             asp_count=aspects_number,\n",
        "             maxlen=max_len,\n",
        "             init_aspects_matrix=np.array([w2v_model.wv[word] for word in ['задания', 'теория', 'преподаватель', 'технологии', 'актуальность']]),\n",
        "             ortho_reg=1).to('cuda')\n",
        "\n",
        "criterion = nn.MSELoss(reduction=\"sum\")\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "for t in range(epochs):\n",
        "    print(\"Epoch %d/%d\" % (t + 1, epochs))\n",
        "    data_iterator = read_data_tensors(df, w2v_model, batch_size=batch_size, maxlen=max_len)\n",
        "\n",
        "    for item_number, (x, texts) in enumerate(data_iterator):\n",
        "        if x.shape[0] < batch_size:  # pad with 0 if smaller than batch size\n",
        "            x = np.pad(x, ((0, batch_size - x.shape[0]), (0, 0), (0, 0)))\n",
        "\n",
        "        x = torch.from_numpy(x).to('cuda')\n",
        "\n",
        "        # extracting bad samples from the very same batch; not sure if this is OK, so todo\n",
        "        negative_samples = torch.stack(\n",
        "            tuple([x[torch.randperm(x.shape[0])[:neg]]\n",
        "                    for _ in range(batch_size)])).to('cuda')\n",
        "\n",
        "        # prediction\n",
        "        y_pred = model(x, negative_samples)\n",
        "\n",
        "        # error computation\n",
        "        loss = criterion(y_pred, y)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print(\"%d batches, and LR: %.5f\" % (item_number, optimizer.param_groups[0]['lr']))\n",
        "    for i, aspect in enumerate(model.get_aspect_words(w2v_model)):\n",
        "        print(\"[%d] %s\" % (i + 1, ' '.join([word[0] for word in aspect])))\n",
        "\n",
        "    print(\"Loss: %.4f\" % loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_aspect_words(w2v_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Us08UBDu5uYg",
        "outputId": "4f7c51ab-cdf3-4c6c-f076-34ebd7740534"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['переменные', 'позновательный', 'крутейшие', 'политика', 'актуальность']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model.wv.vectors.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNqjL7RT8GRC",
        "outputId": "029a7990-ceb7-4be5-915c-aeeea752d5f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12155, 200)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max([len(s.split()) for s in df])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAIZrGuO8RTd",
        "outputId": "887184a4-985a-4cde-b426-f10553800bc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "166"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "0H3OCWvs2LfV",
        "outputId": "95a9bfc5-477d-4c15-ea61-aa558b0e9965"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'александр михаил спасибо курс вашу работу вообще научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными научными'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "vector2text(w2v_model, x[5].cpu().detach().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuxpZurRirbQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04cc6247-f42e-4f0f-b159-b5734df021d0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[8.2009927e-07, 4.5371456e-07, 2.2484665e-04, 9.8814535e-01,\n",
              "        1.3403793e-06, 1.5802808e-04, 1.1401154e-02, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07, 4.2676632e-07, 4.2676632e-07,\n",
              "        4.2676632e-07, 4.2676632e-07]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "attention_weights, aspects_importances, weighted_text_emb = [a.cpu().detach().numpy() for a in model.get_aspects_importances(x[5:6])]\n",
        "attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_MU8Lau2LfV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "i99BekPyjB-I"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}